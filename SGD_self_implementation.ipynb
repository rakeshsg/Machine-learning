{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " SGD self-implementation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eiDWcM_MC3H"
      },
      "source": [
        "# <font color='blue'>Implementing SGD Classifier with Logloss and L2 regularization Using SGD without using sklearn</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfe2NTQtLq11"
      },
      "source": [
        "**Every Grader function has to return True.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk5DSPCLxqT-"
      },
      "source": [
        "<font color='red'> Importing packages</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42Et8BKIxnsp"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import linear_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpSk3WQBx7TQ"
      },
      "source": [
        "<font color='red'>Creating custom dataset</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsMp0oWzx6dv"
      },
      "source": [
        "X, y = make_classification(n_samples=50000, n_features=15, n_informative=10, n_redundant=5,\n",
        "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)\n",
        "# make_classification is used to create custom dataset \n",
        "# link:(https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) for more details"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8W2fg1cyGdX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70bacd8a-323a-4e0a-fbaa-d1f531100f67"
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 15), (50000,))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x99RWCgpqNHw"
      },
      "source": [
        "<font color='red'>Splitting data into train and test </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Kh4dBfVyJMP"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gONY1YiDq7jD"
      },
      "source": [
        "# Standardizing the data.\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DR_YMBsyOci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ac5a5d7-aff0-4615-f5c4-c14f3c5af70c"
      },
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((37500, 15), (37500,), (12500, 15), (12500,))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTkms0XVgCdJ",
        "outputId": "c7d20fa2-1662-4cc5-a349-34e08604139a"
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.39348337, -0.19771903, -0.15037836, -0.21528098, -1.28594363,\n",
              "       -0.66049132,  0.04140556, -0.22680269, -0.511055  , -0.42871073,\n",
              "        0.4210912 ,  0.22560347, -0.6624427 , -0.68888516,  0.56015427])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW4OHswfqjHR"
      },
      "source": [
        "# <font color='red' size=5>SGD classifier</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HpvTwDHyQQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b40293a4-8e59-44f8-f3be-d04a371e6bb2"
      },
      "source": [
        "# alpha : float\n",
        "# Constant that multiplies the regularization term. \n",
        "\n",
        "# eta0 : double\n",
        "# The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules.\n",
        "\n",
        "clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
        "\n",
        "# documentation (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
              "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
              "              fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
              "              loss='log', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
              "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
              "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYaVyQ2lyXcr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c503a048-f542-4151-8eee-e47062ebdfa0"
      },
      "source": [
        "clf.fit(X=X_train, y=y_train) # fitting our model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 1\n",
            "Norm: 0.70, NNZs: 15, Bias: -0.501317, T: 37500, Avg. loss: 0.552526\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 1.04, NNZs: 15, Bias: -0.752393, T: 75000, Avg. loss: 0.448021\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 1.26, NNZs: 15, Bias: -0.902742, T: 112500, Avg. loss: 0.415724\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 1.43, NNZs: 15, Bias: -1.003816, T: 150000, Avg. loss: 0.400895\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 1.55, NNZs: 15, Bias: -1.076296, T: 187500, Avg. loss: 0.392879\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 1.65, NNZs: 15, Bias: -1.131077, T: 225000, Avg. loss: 0.388094\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 1.73, NNZs: 15, Bias: -1.171791, T: 262500, Avg. loss: 0.385077\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 1.80, NNZs: 15, Bias: -1.203840, T: 300000, Avg. loss: 0.383074\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 1.86, NNZs: 15, Bias: -1.229563, T: 337500, Avg. loss: 0.381703\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 1.90, NNZs: 15, Bias: -1.251245, T: 375000, Avg. loss: 0.380763\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 1.94, NNZs: 15, Bias: -1.269044, T: 412500, Avg. loss: 0.380084\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 1.98, NNZs: 15, Bias: -1.282485, T: 450000, Avg. loss: 0.379607\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 2.01, NNZs: 15, Bias: -1.294386, T: 487500, Avg. loss: 0.379251\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 2.03, NNZs: 15, Bias: -1.305805, T: 525000, Avg. loss: 0.378992\n",
            "Total training time: 0.13 seconds.\n",
            "Convergence after 14 epochs took 0.13 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
              "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
              "              fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
              "              loss='log', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
              "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
              "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAfkVI6GyaRO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "508fb925-34e7-4910-d8ed-d4ad243dddf2"
      },
      "source": [
        "clf.coef_, clf.coef_.shape, clf.intercept_\n",
        "#clf.coef_ will return the weights\n",
        "#clf.coef_.shape will return the shape of weights\n",
        "#clf.intercept_ will return the intercept term"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-0.89007184,  0.63162363, -0.07594145,  0.63107107, -0.38434375,\n",
              "          0.93235243, -0.89573521, -0.07340522,  0.40591417,  0.4199991 ,\n",
              "          0.24722143,  0.05046199, -0.08877987,  0.54081652,  0.06643888]]),\n",
              " (1, 15),\n",
              " array([-1.30580538]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-CcGTKgsMrY"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "## <font color='red' size=5> Implementing Logistic Regression with L2 regularization Using SGD: without using sklearn </font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU2Y3-FQuJ3z"
      },
      "source": [
        "\n",
        "<br>\n",
        "\n",
        "* Initialize the weight_vector and intercept term to zeros (code in <font color='blue'>def initialize_weights()</font>)\n",
        "\n",
        "* Create a loss function (code in <font color='blue'>def logloss()</font>) \n",
        "\n",
        " $log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$\n",
        "- for each epoch:\n",
        "\n",
        "    - for each batch of data points in train: (keep batch size=1)\n",
        "\n",
        "        - calculate the gradient of loss function w.r.t each weight in weight vector (code in <font color='blue'>def gradient_dw()</font>)\n",
        "\n",
        "        $dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)})$ <br>\n",
        "\n",
        "        - Calculate the gradient of the intercept (code in <font color='blue'> def gradient_db()</font>) <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>check this</a>(Reference)\n",
        "\n",
        "           $ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t}))$\n",
        "\n",
        "        - Update weights and intercept (check the equation number 32 in the above mentioned <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>pdf</a>(reference): <br>\n",
        "        $w^{(t+1)}← w^{(t)}+α(dw^{(t)}) $<br>\n",
        "\n",
        "        $b^{(t+1)}←b^{(t)}+α(db^{(t)}) $\n",
        "    - calculate the log loss for train and test with the updated weights\n",
        "    - And if you wish, you can compare the previous loss and the current loss, if it is not updating, then\n",
        "        you can stop the training\n",
        "    - append this loss in the list ( this will be used to see how loss is changing for each epoch after the training is over )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR_HgjgS_wKu"
      },
      "source": [
        "<font color='blue'>Initialize weights </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GecwYV9fsKZ9"
      },
      "source": [
        "def initialize_weights(dim):\n",
        "    ''' In this function, we will initialize our weights and bias'''\n",
        "    #initialize the weights to zeros array of (1,dim) dimensions\n",
        "    weight=[]\n",
        "    b=0\n",
        "    for i in range(0,dim):\n",
        "      weight.append(0)\n",
        "    #w=np.array(weight)\n",
        "    return weight,b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7I6uWBRsKc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b13415bc-24c9-4177-a61c-f94367193ea4"
      },
      "source": [
        "dim=X_train[0]\n",
        "w,b = initialize_weights(len(dim)) #initializing weights to 0 which is of dimension (1,15) same as our number of features and bias(intercept)=0\n",
        "print('w =',(w))\n",
        "print('b =',str(b))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "b = 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MI5SAjP9ofN"
      },
      "source": [
        "<font color='cyan'>Grader function - 1 </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv1llH429wG5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20135d6e-52d2-41c6-d522-3679b4652cf3"
      },
      "source": [
        "dim=X_train[0] \n",
        "w,b = initialize_weights(len(dim))\n",
        "def grader_weights(w,b):\n",
        "  assert((len(w)==len(dim)) and b==0 and np.sum(w)==0.0)\n",
        "  return True\n",
        "grader_weights(w,b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN83oMWy_5rv"
      },
      "source": [
        "<font color='blue'>Compute sigmoid </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPv4NJuxABgs"
      },
      "source": [
        "$sigmoid(z)= 1/(1+exp(-z))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAfmQF47_Sd6"
      },
      "source": [
        "import math\n",
        "def sigmoid(z):\n",
        "    ''' In this function, we will return sigmoid of z'''\n",
        "    sig=1/(1+math.exp(-z))\n",
        "    return sig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YrGDwg3Ae4m"
      },
      "source": [
        "<font color='cyan'>Grader function - 2</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_JASp_NAfK_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b80b19d4-dd12-4aa7-b0ff-8bdd045d3775"
      },
      "source": [
        "def grader_sigmoid(z):\n",
        "  val=sigmoid(z)\n",
        "  assert(val==0.8807970779778823)\n",
        "  return True\n",
        "grader_sigmoid(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS7JXbcrBOFF"
      },
      "source": [
        "<font color='blue'> Compute loss </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfEiS22zBVYy"
      },
      "source": [
        "$log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaFDgsp3sKi6"
      },
      "source": [
        "def logloss(y_true,y_pred):\n",
        "    '''In this function, we will compute log loss '''\n",
        "    sum=0\n",
        "    for i in range(0,len(y_true)):\n",
        "      a=math.log10(y_pred[i])\n",
        "      b=float(y_true[i])*a\n",
        "      c=math.log10(1-y_pred[i])\n",
        "      d=float((1-y_true[i]))*c\n",
        "      e=b+d\n",
        "      sum=sum+e\n",
        "    loss=(-1)*(1/len(y_true))*sum\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs1BTXVSClBt"
      },
      "source": [
        "<font color='cyan'>Grader function - 3 </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzttjvBFCuQ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b031c5f-5daf-4798-de6c-2ea227a8b493"
      },
      "source": [
        "def grader_logloss(true,pred):\n",
        "  loss=logloss(true,pred)\n",
        "  assert(loss==0.07644900402910389)\n",
        "  return True\n",
        "true=[1,1,0,1,0]\n",
        "pred=[0.9,0.8,0.1,0.8,0.2]\n",
        "grader_logloss(true,pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQabIadLCBAB"
      },
      "source": [
        "<font color='blue'>Compute gradient w.r.to  'w' </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTMxiYKaCQgd"
      },
      "source": [
        "$dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)}$ <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMVikyuFsKo5"
      },
      "source": [
        "def gradient_dw(x,y,w,b,alpha,N):\n",
        "    '''In this function, we will compute the gardient w.r.to w '''\n",
        "    summ=0\n",
        "    z=0\n",
        "    for i in range(0,len(w)):\n",
        "        summ=summ+(w[i]*x[i])\n",
        "    z=summ+b\n",
        "    sig=sigmoid(z)\n",
        "    f=y-sig\n",
        "    arr=x*f\n",
        "      #print(arr_previous)  #till here its calculation of first term xn(yn−σ((w(t))Txn+bt))\n",
        "    m=alpha/N  # alpha is 0.0001\n",
        "    n=np.array(w)\n",
        "    mul=(n*m) #till here its calculation of second term −λNw(t)\n",
        "    dw=arr-mul #Adding first and second term\n",
        "    return dw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUFLNqL_GER9"
      },
      "source": [
        "<font color='cyan'>Grader function - 4 </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI3xD8ctGEnJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8432127-3b65-41de-f125-18ffcfb93fd5"
      },
      "source": [
        "def grader_dw(x,y,w,b,alpha,N):\n",
        "  grad_dw=gradient_dw(x,y,w,b,alpha,N)\n",
        "  assert(np.sum(grad_dw)==2.613689585)\n",
        "  return True\n",
        "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
        "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
        "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
        "grad_y=0\n",
        "grad_w,grad_b=initialize_weights(len(grad_x)) #my initialize weights returns 0 and 0 for both w and b\n",
        "alpha=0.0001\n",
        "N=len(X_train)\n",
        "grader_dw(grad_x,grad_y,w,b,alpha,N)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE8g84_GI62n"
      },
      "source": [
        "<font color='blue'>Compute gradient w.r.to 'b' </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHvTYZzZJJ_N"
      },
      "source": [
        "$ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nUf2ft4EZp8"
      },
      "source": [
        " def gradient_db(x,y,w,b):\n",
        "     #'''In this function, we will compute gradient w.r.to b '''\n",
        "   db=0\n",
        "   sumb=0 \n",
        "   for i in range(0,len(w)):\n",
        "        sumb=sumb+(w[i]*x[i])\n",
        "   z1=sumb+b\n",
        "   sig=sigmoid(z1)\n",
        "   db=y-sig\n",
        "   return db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbcBzufVG6qk"
      },
      "source": [
        "<font color='cyan'>Grader function - 5 </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfFDKmscG5qZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25abcbef-83e2-44de-ea68-841505eb238d"
      },
      "source": [
        "def grader_db(x,y,w,b):\n",
        "  grad_db=gradient_db(x,y,w,b)\n",
        "  assert(grad_db==-0.5)\n",
        "  return True\n",
        "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
        "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
        "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
        "grad_y=0\n",
        "grad_w,grad_b=initialize_weights(len(grad_x))\n",
        "alpha=0.0001\n",
        "N=len(X_train)\n",
        "grader_db(grad_x,grad_y,grad_w,grad_b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCK0jY_EOvyU"
      },
      "source": [
        "<font color='blue'> Implementing logistic regression</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmAdc5ejEZ25"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def train(X_train,y_train,X_test,y_test,epochs,alpha,eta0):\n",
        "    ''' In this function, we will implement logistic regression'''\n",
        "    #Here eta0 is learning rate\n",
        "    #implement the code as follows\n",
        "    # initalize the weights (call the initialize_weights(X_train[0]) function)\n",
        "    w,b=initialize_weights(len(X_train[0]))\n",
        "    train_loss=[]\n",
        "    test_loss=[]\n",
        "    epoch=[]\n",
        "    loss_train_prev=0\n",
        "    # for every epoch\n",
        "    for i in range(0,epochs):\n",
        "      for j in range(0,len(X_train)):\n",
        "        dw=gradient_dw(X_train[j],y_train[j],w,b,alpha,N)\n",
        "        db=gradient_db(X_train[j],y_train[j],w,b)                       \n",
        "        w=w+(eta0*dw)           #update w, b                      \n",
        "        b=b+(eta0*db)                                        \n",
        "      train_pred_op=pred(w,b,X_train)\n",
        "      train_loss.append(logloss(y_train,train_pred_op))\n",
        "      loss_train=logloss(y_train,train_pred_op)\n",
        "      epoch.append(i)\n",
        "      test_pred_op=pred(w,b,X_test)\n",
        "      test_loss.append(logloss(y_test,test_pred_op))\n",
        "      if abs(loss_train-loss_train_prev)<1e-4:\n",
        "        break\n",
        "      loss_train_prev=loss_train\n",
        "    plt.xlabel(\"epoch number\")\n",
        "    plt.ylabel(\"Loss(log loss)\")\n",
        "    plt.title('epoch number vs train , test loss')\n",
        "    plt.plot(epoch, train_loss,label = \"Train loss\")\n",
        "    plt.plot(epoch, test_loss,label = \"Test loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "        # predict the output of x_train[for all data points in X_train] using w,b\n",
        "        #compute the loss between predicted and actual values (call the loss function)\n",
        "        # store all the train loss values in a list\n",
        "        # predict the output of x_test[for all data points in X_test] using w,b\n",
        "        #compute the loss between predicted and actual values (call the loss function)\n",
        "        # store all the test loss values in a list\n",
        "        # you can also compare previous loss and current loss, if loss is not updating then stop the process and return w,b\n",
        "\n",
        "    return w,b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUquz7LFEZ6E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "a4f2a046-1573-4989-d1f2-fd20784d67bf"
      },
      "source": [
        "alpha=0.0001\n",
        "eta0=0.0001\n",
        "N=len(X_train)\n",
        "epochs=50\n",
        "w,b=train(X_train,y_train,X_test,y_test,epochs,alpha,eta0)\n",
        "print(\"w: \",w)\n",
        "print(\"b: \",b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w:  [-0.89482323  0.63922609 -0.07409042  0.63113611 -0.38279876  0.9346933\n",
            " -0.89664514 -0.07124397  0.41113377  0.41550075  0.24845771  0.05300616\n",
            " -0.08703024  0.53952896  0.06749254]\n",
            "b:  -1.3030058566516547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0I8Ts0jQggF"
      },
      "source": [
        "If we compare this result with below values of w and b which was implemented by scikit learn, they are very close. So we can conclude our implementation is correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nx8Rs9rfEZ1R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccb13581-ffa1-4d52-df84-135a864a57c5"
      },
      "source": [
        "# these are the results we got after we implemented sgd and found the optimal weights and intercept\n",
        "w-clf.coef_, b-clf.intercept_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-4.75139040e-03,  7.60245639e-03,  1.85102713e-03,\n",
              "          6.50362355e-05,  1.54498740e-03,  2.34086809e-03,\n",
              "         -9.09928936e-04,  2.16124544e-03,  5.21959720e-03,\n",
              "         -4.49834999e-03,  1.23628554e-03,  2.54417563e-03,\n",
              "          1.74962845e-03, -1.28756176e-03,  1.05365463e-03]]),\n",
              " (1, 15),\n",
              " array([0.00279952]))"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ]
}